"""
Pipeline artifact schema for multi-agent systems.

From Listing 3.2 in Black Hat AI.

Artifacts are the connective tissue of every pipeline: they preserve context,
enforce traceability, and allow interrupted runs to resume exactly where they stopped.
"""

from pydantic import BaseModel, Field
from datetime import datetime
from typing import Dict, Any, Optional
import uuid


class PipelineArtifact(BaseModel):
    """
    Structured record describing a pipeline stage execution.

    Artifacts contain both the model responses and metadata about the agent run,
    including what the agent did, what it received, and what it produced.

    Attributes:
        run_id: Unique identifier grouping all artifacts from a single execution
        stage: Name of the agent/stage that produced this artifact
        input: The context/data received by the stage
        output: The result generated by the stage
        success: Whether the execution completed successfully
        timestamp: When this artifact was created (UTC)
        error: Optional error message if execution failed
        meta: Optional additional metadata

    Example:
        artifact = PipelineArtifact(
            run_id="74bfe8c0f5c84c16b7d90a2c334c9e0b",
            stage="triage",
            input={"targets": ["admin.example.com", "cdn.example.com"]},
            output={"high_risk": ["admin.example.com"]},
            success=True
        )
    """

    run_id: str = Field(default_factory=lambda: uuid.uuid4().hex)
    stage: str
    input: Dict[str, Any]
    output: Dict[str, Any]
    success: bool
    timestamp: datetime = Field(default_factory=datetime.utcnow)
    error: Optional[str] = None
    meta: Optional[Dict[str, Any]] = None

    def to_jsonl_record(self) -> Dict[str, Any]:
        """
        Convert artifact to a JSON-serializable dictionary for JSONL logging.

        Returns:
            Dictionary with ISO-formatted timestamp suitable for JSON serialization.
        """
        data = self.model_dump()
        data["timestamp"] = self.timestamp.isoformat()
        return data

    @classmethod
    def from_previous(
        cls,
        previous: Optional["PipelineArtifact"],
        stage: str,
        output: Dict[str, Any],
        success: bool = True,
        error: Optional[str] = None,
        meta: Optional[Dict[str, Any]] = None,
    ) -> "PipelineArtifact":
        """
        Create a new artifact that continues from a previous one.

        This maintains the run_id across stages for traceability.

        Args:
            previous: The artifact from the previous stage (or None for first stage)
            stage: Name of the current stage
            output: Output data from this stage
            success: Whether this stage succeeded
            error: Optional error message
            meta: Optional metadata

        Returns:
            New PipelineArtifact with the same run_id as previous
        """
        run_id = previous.run_id if previous else uuid.uuid4().hex
        input_data = previous.output if previous else {}

        return cls(
            run_id=run_id,
            stage=stage,
            input=input_data,
            output=output,
            success=success,
            error=error,
            meta=meta,
        )
